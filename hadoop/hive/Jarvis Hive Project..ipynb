{
  "metadata": {
    "name": "Jarvis Hive Project",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Query GS data\n- Create an external Hive table\n- Write a query that counts the number of rows\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_gs;"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "CREATE EXTERNAL TABLE wdi_gs\r\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\r\nROW FORMAT DELIMITED FIELDS TERMINATED BY \u0027,\u0027 LINES TERMINATED BY \u0027\\n\u0027\r\nLOCATION \u0027gs://jarvis_data_eng_zach/datasets/wdi_2016\u0027\r\nTBLPROPERTIES (\"skip.header.line.count\"\u003d\"1\");"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DESCRIBE FORMATTED wdi_gs;"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_gs;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Hive Project"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Load GS to HDFS\n- HDFS home directory: `/user/ztbarlow66`\n\n### External Table `wdi_csv_text`"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_csv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nCREATE EXTERNAL TABLE wdi_csv_text\n    (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY \u0027,\u0027 LINES TERMINATED BY \u0027\\n\u0027\nLOCATION \u0027hdfs:///user/ztbarlow66/hive/wdi/wdi_csv_text\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "FROM wdi_gs\nINSERT OVERWRITE TABLE wdi_csv_text\nSELECT *;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Foundings** `SELECT COUNT(countryName) FROM wdi_csv_text;`\n- first select: 161.386 seconds\n- second time: 8.236 seconds\n\n*clearing filesystem cache* [on master and both workers]\n- running query: 51.29 seconds\n\n*Note:* these times are from overall job time and not just map reduce time."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Hive vs Bash\n-Copy `wdi_csv_text` HDFS file to the master node and count the number of rows"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n#SSH to master node\ncd ~\nhdfs  dfs -get  hdfs:///user/ztbarlow66/hive/wdi/wdi_csv_text .\ncd wdi_csv_text\n#calculate current directory size\ndu -ch .\n#1.8G\ttotal\n\n#clear fs cache\necho 3 | sudo tee /proc/sys/vm/drop_caches\n#bash row count\ndate +%s \u0026\u0026 cat * | wc \u0026\u0026 date +%s\n\n# 1621001307\n# 1621001328 -\u003e 19 seconds"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Performance Analysis\nAs we can see there is a difference in running Hive and Bash commands to check the number of rows in the files.\nThe fastest **Hive** time being *8.236 secs* after the data has been cached is the quickest and slightly behind is the **bash** check at *19 secs*. This is where Hive will be so extremely useful because the more data will slow down bash but not really affect Hive\u0027s speed."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Parsing Issue"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT distinct(indicatorcode)\nFROM wdi_csv_text\nORDER BY indicatorcode\nLIMIT 20;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "A possible issue to this problem is that the indicatorname column has a limit on it\u0027s characters and is being split to the two columns and overriding the data already in the indicatorcode column.\n\nBelow is the creation of a debug table and then the output of the table to find an issue."
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_gs_debug;\n\nCREATE EXTERNAL TABLE wdi_gs_debug \n    (line String)\nROW FORMAT DELIMITED LINES TERMINATED BY \u0027\\n\u0027\nLOCATION \u0027gs://jarvis_data_eng_zach/datasets/wdi_2016\u0027\nTBLPROPERTIES (\"skip.header.line.count\"\u003d\"1\");\n\nDESCRIBE FORMATTED wdi_gs_debug;"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT line \nFROM wdi_gs_debug\nWHERE line LIKE \"%\\(\\% of urban population\\)\\\"%\""
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "From this it seems to be parsed on the comma within the indicatorname. For example \"Access to electricity, urban (% of urban population)\", it would possibly be split at the comma after electricity."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_gs;\nCREATE EXTERNAL TABLE IF NOT EXISTS wdi_opencsv_gs\n    (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \u0027gs://jarvis_data_eng_zach/datasets/wdi_2016\u0027"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_text;\n\nCREATE EXTERNAL TABLE IF NOT EXISTS wdi_opencsv_text\n    (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \"hdfs:///user/ztbarlow66/hive/wdi/wdi_opencsv_text\""
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_opencsv_text SELECT * FROM wdi_opencsv_gs;"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT distinct(indicatorcode) FROM wdi_opencsv_text;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Difference between run times of queries\n- Block 1: 20secs\n- Block 2: 2 min 54 sec\n\nThe non-OpenCSVSerDe is a lot faster than the OpenCSVSerDe. This could be due to OpenCSVSerDe being a more complicated SerDe, and when it is run on each line in the files, it will cause an increase in job execution time."
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) from wdi_csv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryname) FROM wdi_opencsv_text;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## OpenCSVSerde limitaion\n\nAs we can see from the descriptions the OpenCSVSerDe turns each column into a string from a deserializer, the only way to get the desired type is to create a view of the table. This is what is causing the time difference between the two queries."
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DESCRIBE FORMATTED wdi_opencsv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DESCRIBE FORMATTED wdi_csv_text;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Create a view of the text view to query off of and show the data in their actual format."
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP VIEW IF EXISTS wdi_opencsv_text_view;\n\nCREATE VIEW IF NOT EXISTS wdi_opencsv_text_view\nAS\nSELECT CAST(year AS INT), countryname, countrycode, indicatorname, indicatorcode, CAST(indicatorvalue AS FLOAT) \nFROM wdi_opencsv_text"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DESCRIBE FORMATTED wdi_opencsv_text_view;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2015 Canada GDP Growth HQL\n\nThe query took 2 min 1 secs. The reason is due to the conditions on the date, having to check each line that has a name that is like what we are looking for, slowing down the process. This can be optimized using partitions, if we partition by date and country, we only have to scan those files with the partitions we are looking for instead "
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorvalue AS GDP_growth_value, year, countryname\nFROM wdi_opencsv_text_view\nWHERE year \u003d 2015 \n    AND countrycode \u003d \u0027CAN\u0027 \n    AND indicatorname LIKE \"GDP growth \\(annual \\%\\)%\";"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Hive Partitions\n- Block 1: Creation of partitions table using OpenCSVSerDe\n- Block 2: Setting modes and restrictions on the mode\n- Block 3: Inserting data into the partitions table using dynamic partitions\n- Block 4: Running query"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_opencsv_text_partitions;\nCREATE TABLE wdi_opencsv_text_partitions\n    (countryname STRING, countrycode STRING, indicatorname STRING, indicatorcode STRING, indicatorvalue FLOAT)\nPARTITIONED BY (year INTEGER)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \"hdfs:///user/ztbarlow66/hive/wdi/wdi_opencsv_text_partitions\""
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SET hive.exec.dynamic.partition.mode\u003dnonstrict;\nSET hive.stats.column.autogather\u003dfalse; \nSET hive.optimize.sort.dynamic.partition\u003dtrue;\n-- SET hive.auto.convert.join\u003dfalse;"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "FROM wdi_opencsv_text wdt\nINSERT OVERWRITE TABLE wdi_opencsv_text_partitions PARTITION(year)\nSELECT wdt.countryname, wdt.countrycode, wdt.indicatorname, wdt.indicatorcode, wdt.indicatorvalue, wdt.year;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorvalue AS GDP_growth_value, year, countryname\nFROM wdi_opencsv_text_partitions\nWHERE year \u003d 2015 \n    AND countrycode \u003d \u0027CAN\u0027 \n    AND indicatorname LIKE \"GDP growth \\(annual \\%\\)%\";"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Without partitions : 2 min 1 secs\nWith partitions : 20 secs.\n\nThis performance improvement is entirely due to partitioning on the year which filters the amount of data we need to read in order to run the job. We could potentially partition by country too but it is not really necessary at the moment."
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -ls hdfs:///user/ztbarlow66/hive/wdi/wdi_opencsv_text_partitions | wc -l"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Columnar File Optimization"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "DROP TABLE IF EXISTS wdi_csv_parquet;\nCREATE EXTERNAL TABLE wdi_csv_parquet\n    (year INTEGER, countryName STRING, countryCode STRING, \n    indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nSTORED AS PARQUET\nLOCATION \"hdfs:///user/ztbarlow66/hive/wdi/wdi_csv_parquet\""
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "INSERT OVERWRITE TABLE wdi_csv_parquet\nSELECT * FROM wdi_opencsv_gs;"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -du -h hdfs:///user/ztbarlow66/hive/wdi/"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_csv_parquet;"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT count(countryName) FROM wdi_opencsv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorvalue AS GDP_growth_value, year, countryname\nFROM wdi_csv_parquet\nWHERE year \u003d 2015 \n    AND countrycode \u003d \u0027CAN\u0027 \n    AND indicatorname LIKE \"GDP growth \\(annual \\%\\)%\";"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT indicatorvalue AS GDP_growth_value, year, countryname\nFROM wdi_opencsv_text\nWHERE year \u003d 2015 \n    AND countrycode \u003d \u0027CAN\u0027 \n    AND indicatorname LIKE \"GDP growth \\(annual \\%\\)%\";"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "From both of the queries, we see that the parquet runs them both in 15 and 16 seconds whereas the text file is ran in 1min 24sec for both. If we look at the file sizes of the two, the parquet is size of 137.2 M and the text is of size 2.3 G which is a huge reason for the perfomance difference."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Highest GDP Growth\n- Block 1: Query to find max gdp growth for each country using Hive\n- Block 2: Query to find max gdp growth for each country using Spark SQL"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT T1.indicatorvalue AS GDP_growth_value, T1.year, T1.countryname\nFROM wdi_csv_parquet T1\nINNER JOIN (\n    SELECT MAX(indicatorvalue) AS GDP_growth_value, countryname\n    FROM wdi_csv_parquet\n    WHERE indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\n    GROUP BY countryname\n) as T2\nON T1.indicatorvalue \u003d T2.GDP_growth_value AND T1.countryname \u003d T2.countryname\nORDER BY T1.indicatorvalue DESC"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT T1.indicatorvalue AS GDP_growth_value, T1.year, T1.countryname\nFROM wdi_csv_parquet T1\nINNER JOIN (\n    SELECT MAX(indicatorvalue) AS GDP_growth_value, countryname\n    FROM wdi_csv_parquet\n    WHERE indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\n    GROUP BY countryname\n) as T2\nON T1.indicatorvalue \u003d T2.GDP_growth_value AND T1.countryname \u003d T2.countryname\nORDER BY T1.indicatorvalue DESC"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Sort GDP by country and year"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "SELECT countryname, year, indicatorcode, indicatorvalue as GDP_growth_value\nFROM wdi_csv_parquet\nWHERE indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\nORDER BY countryname DESC, year ASC"
    }
  ]
}